{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle       \nimport re\nimport tensorflow as tf\nimport torch.nn as nn\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.stem import PorterStemmer\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-29T16:22:21.083832Z","iopub.execute_input":"2022-12-29T16:22:21.084302Z","iopub.status.idle":"2022-12-29T16:22:30.315771Z","shell.execute_reply.started":"2022-12-29T16:22:21.084214Z","shell.execute_reply":"2022-12-29T16:22:30.314751Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nEncoder Architecture Inputs:2(embedding,hidden_state) , Outputs:2(enc_output,thought_vector)\n\nENCODER_CLASS:\n\nWord2Vec(Inputs) => Encoder(Vocab_size,embedding from word2vec,GRU's required,words/sentences to fetched once) \n\n=> Encoder_Outputs,Thought Vector \n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:48:57.031946Z","iopub.execute_input":"2022-12-29T10:48:57.032514Z","iopub.status.idle":"2022-12-29T10:48:57.040585Z","shell.execute_reply.started":"2022-12-29T10:48:57.032463Z","shell.execute_reply":"2022-12-29T10:48:57.039416Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"\"\\nEncoder Architecture Inputs:2(embedding,hidden_state) , Outputs:2(enc_output,thought_vector)\\n\\nENCODER_CLASS:\\n\\nWord2Vec(Inputs) => Encoder(Vocab_size,embedding from word2vec,GRU's required,words/sentences to fetched once) \\n\\n=> Encoder_Outputs,Thought Vector \\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self,vocab_size,embedding,encoder_units,batch_size):\n        super(Encoder,self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = encoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding)\n        self.gru = tf.keras.layers.GRU(self.enc_units,return_sequences=True,return_state=True,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self,inputs,hidden_state):\n        embedded_inputs = self.embedding(inputs)\n        enc_output,thought_vector= self.gru(embedded_inputs,initial_state=hidden_state)\n        return enc_output,thought_vector","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:48:57.042699Z","iopub.execute_input":"2022-12-29T10:48:57.043109Z","iopub.status.idle":"2022-12-29T10:48:57.063407Z","shell.execute_reply.started":"2022-12-29T10:48:57.043054Z","shell.execute_reply":"2022-12-29T10:48:57.062628Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nAttention_network_Architecture: Inputs:2(enc_outputs,thought_vector), Outputs:2(attentn_output,attent_weight)\n                                                                                                     \nEnc_outputs   --> Enc_Layer -> ------                                                   \n                                      Final_layer(Activation(++)) --> Attention_weights * encoder_output--2\nThought_vector--> Thought_layer -> ---\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:01.998713Z","iopub.execute_input":"2022-12-29T10:49:01.999052Z","iopub.status.idle":"2022-12-29T10:49:02.005067Z","shell.execute_reply.started":"2022-12-29T10:49:01.999024Z","shell.execute_reply":"2022-12-29T10:49:02.004020Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'\\nAttention_network_Architecture: Inputs:2(enc_outputs,thought_vector), Outputs:2(attentn_output,attent_weight)\\n                                                                                                     \\nEnc_outputs   --> Enc_Layer -> ------                                                   \\n                                      Final_layer(Activation(++)) --> Attention_weights * encoder_output--2\\nThought_vector--> Thought_layer -> ---\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Attention,self).__init__()\n        self.enc_output_layer = tf.keras.layers.Dense(units,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.thought_layer = tf.keras.layers.Dense(units,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.final_output = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n    \n    def call(self,enc_output,thought_vector):\n        thought_matrix = tf.expand_dims(thought_vector,1)    #return tensor, add extra tensor dimension to the input\n        scores = self.final_output(tf.keras.activations.tanh(self.enc_output_layer(enc_output)+self.thought_layer(thought_matrix)))\n        attention_weights = tf.keras.activations.softmax(scores,axis=-1)\n        \n        attention_output = attention_weights * enc_output\n        attention_output = tf.reduce_sum(attention_output, axis=1)   #new_shape (batch_size,attention_outputs)\n        return attention_output,attention_weights        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:02.393377Z","iopub.execute_input":"2022-12-29T10:49:02.393957Z","iopub.status.idle":"2022-12-29T10:49:02.401828Z","shell.execute_reply.started":"2022-12-29T10:49:02.393919Z","shell.execute_reply":"2022-12-29T10:49:02.401152Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDecoder Archirecture: Inputs:2(hindi_words,dec_hidden,enc_outputs) Outputs:2(dec_output,dec_hidden)\n\n1. embedding_output(hindi_inputs) + attention_outputs\n\n\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:02.784449Z","iopub.execute_input":"2022-12-29T10:49:02.785053Z","iopub.status.idle":"2022-12-29T10:49:02.792063Z","shell.execute_reply.started":"2022-12-29T10:49:02.785013Z","shell.execute_reply":"2022-12-29T10:49:02.790534Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\nDecoder Archirecture: Inputs:2(hindi_words,dec_hidden,enc_outputs) Outputs:2(dec_output,dec_hidden)\\n\\n1. embedding_output(hindi_inputs) + attention_outputs\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self,vocab_size,embedding,decoder_units,batch_size):\n        super(Decoder,self).__init__()\n        \n        self.batch_size = batch_size\n        self.dec_units = decoder_units\n        self.batch_size = batch_size\n        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding)\n        \n        self.gru = tf.keras.layers.GRU(self.dec_units,return_sequences=True,return_state=True,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n        self.attention = Attention(self.dec_units)\n        self.word_output = tf.keras.layers.Dense(vocab_size,kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, hindi_inputs, enc_outputs, thought_vector):\n        attention_output, attention_weights = self.attention(enc_outputs,thought_vector)\n        embedded_inputs = self.embedding(hindi_inputs)  #shape(batch_size,num_words,size_of_embediing)\n        attention_output = tf.expand_dims(attention_output, 0)  #shape (batch_size,1,size_of_embedding)\n        \n        concat_input = tf.concat([attention_output,embedded_inputs],axis=-1)\n        \n        decoder_outputs,hidden_state = self.gru(concat_input)  #shape(batch_size,1, size_of_embedding)\n        decoder_outputs = tf.reshape(decoder_outputs,(-1,decoder_outputs.shape[2])) #shape(batch_size,sizeof_embed\n        \n        final_output = self.word_output(decoder_outputs)\n        return final_output,hidden_state,attention_weights","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:04.663020Z","iopub.execute_input":"2022-12-29T10:49:04.663689Z","iopub.status.idle":"2022-12-29T10:49:04.673862Z","shell.execute_reply.started":"2022-12-29T10:49:04.663652Z","shell.execute_reply":"2022-12-29T10:49:04.672517Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nModel Training Architecture:\n\ndef loss_function:\n\n1. Loss function: SparseCategoricalCrossentropy\n2. Optimizer: Adam\n\ninput = [Hii I'm samiksha a Data Science Enthusiast]\n\ny_real = [0,0,0,1,0,0,0]\n\nTo improve the GRU's performance we mask the corerctly predicted output and tell's model to focus on incorrect\nprediction values. Hence we mask the Correct Prediction\ny_real = [0,0,0,1,0,0,0] ---> base_mask = [1,1,1,0,1,1,1]\n\nbase_loss = [0.001,0.001,0.001,0.9,0.001,0.001,0.001]\n\nfinal_output = base_loss * base_mask \n\nfinal_output = [0.001,0.001,0.001,0,0.001,0.001,001]\n\ndef Training:\n\nEncoder = _,_  , _,_ = attention_network = attention_weights,attention_outputs,\n[attention_outputs + embedding(hindi_inputs)] , hidden_state => Decoder\nDecoder(decoder_output,dec_hidden)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:05.381676Z","iopub.execute_input":"2022-12-29T10:49:05.382024Z","iopub.status.idle":"2022-12-29T10:49:05.389704Z","shell.execute_reply.started":"2022-12-29T10:49:05.381996Z","shell.execute_reply":"2022-12-29T10:49:05.388022Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\"\\nModel Training Architecture:\\n\\ndef loss_function:\\n\\n1. Loss function: SparseCategoricalCrossentropy\\n2. Optimizer: Adam\\n\\ninput = [Hii I'm samiksha a Data Science Enthusiast]\\n\\ny_real = [0,0,0,1,0,0,0]\\n\\nTo improve the GRU's performance we mask the corerctly predicted output and tell's model to focus on incorrect\\nprediction values. Hence we mask the Correct Prediction\\ny_real = [0,0,0,1,0,0,0] ---> base_mask = [1,1,1,0,1,1,1]\\n\\nbase_loss = [0.001,0.001,0.001,0.9,0.001,0.001,0.001]\\n\\nfinal_output = base_loss * base_mask \\n\\nfinal_output = [0.001,0.001,0.001,0,0.001,0.001,001]\\n\\ndef Training:\\n\\nEncoder = _,_  , _,_ = attention_network = attention_weights,attention_outputs,\\n[attention_outputs + embedding(hindi_inputs)] , hidden_state => Decoder\\nDecoder(decoder_output,dec_hidden)\\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n        \n    \n    def loss_function(self,y_real,y_pred):\n        base_mask = tf.math.logical_not(tf.math.equal(y_real,0))\n        base_loss = self.base_loss_function(y_real,y_pred)\n        \n        cast_mask = tf.cast(base_mask,dtype=base_loss.dtype)\n        final_output = cast_mask * base_loss\n        \n        return tf.reduce_mean(final_output)             #\n    \n    def Training(self,train_data,label_data,enc_hidden,encoder,decoder,batch_size,label_tokenizer):\n        loss = 0\n        \n        with tf.GradientTape() as tape:\n            enc_output,thought_vector = encoder(train_data,enc_hidden)\n            dec_hidden = thought_vector\n            dec_input = tf.expand_dims([label_tokenizer.word_index['start']] * batch_size, 0)\n            \n            # As decoder calculates loss of each word hence to collectively call loss over the sentence.\n            for index in range(1, label_data.shape[1]):\n                outputs,dec_hidden, _ = decoder(dec_input,enc_output,dec_hidden)\n                \n                dec_input = tf.expand_dims(label_data[:,index],1)\n                loss = loss + self.loss_function(label_data[:,index],outputs)\n                \n        word_loss = loss / int(label_data.shape[1])\n        \n        variables = encoder.trainable_variables + decoder.trainable_variables\n        \n        gradients = tape.gradient(loss,variables)\n        self.optimizer.apply_gradients(zip(gradients,variables))\n        \n        return word_loss","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:05.724905Z","iopub.execute_input":"2022-12-29T10:49:05.726365Z","iopub.status.idle":"2022-12-29T10:49:05.736089Z","shell.execute_reply.started":"2022-12-29T10:49:05.726313Z","shell.execute_reply":"2022-12-29T10:49:05.734976Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Data_preprocessing:\n    def __init__(self):\n        self.temp=None\n        \n    def get_data(self,path):\n        file = open(path,'r').read()\n        lists = [f.split('\\t') for f in file.split('\\n')]\n        \n        questions = [x[0] for x in lists]\n        answers = [x[1] for x in lists]\n        \n        return questions,answers\n    \n    def process_sentence(self,line):\n        line = line.lower().strip()\n        \n        line = re.sub(r\"([?!.,])\",\" \",line)\n        line = re.sub(r'[\" \"]+',\" \",line)\n        line = re.sub(r\"[^a-zA-Z?!,.]+\",\" \",line)\n        line = line.strip()\n        line = '<start> '+line+' <end>'\n        return line\n    \n    def lemmatization(self,inputs):\n        word_stemmer = PorterStemmer()\n        inputs = word_stemmer.stem(inputs)\n        \n    def word_2_vec(self,inputs):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(inputs)\n        \n        vectors = tokenizer.texts_to_sequences(inputs)\n        vectors = pad_sequences(vectors,padding = \"post\")\n        \n        return vectors,tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:06.110881Z","iopub.execute_input":"2022-12-29T10:49:06.112115Z","iopub.status.idle":"2022-12-29T10:49:06.120381Z","shell.execute_reply.started":"2022-12-29T10:49:06.112078Z","shell.execute_reply":"2022-12-29T10:49:06.119458Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data = Data_preprocessing()\nquestions,answers = data.get_data('../input/coversational-ai-chatbot/chatbot.txt')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:06.442782Z","iopub.execute_input":"2022-12-29T10:49:06.443123Z","iopub.status.idle":"2022-12-29T10:49:06.464836Z","shell.execute_reply.started":"2022-12-29T10:49:06.443095Z","shell.execute_reply":"2022-12-29T10:49:06.464177Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"max_train_length = max(len(ele) for ele in questions)\nmax_test_length = max(len(ele) for ele in answers)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:06.814321Z","iopub.execute_input":"2022-12-29T10:49:06.814885Z","iopub.status.idle":"2022-12-29T10:49:06.820410Z","shell.execute_reply.started":"2022-12-29T10:49:06.814853Z","shell.execute_reply":"2022-12-29T10:49:06.819806Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"questions = [data.process_sentence(str(sentence)) for sentence in questions]\nanswers = [data.process_sentence(str(sentences)) for sentences in answers]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:07.186518Z","iopub.execute_input":"2022-12-29T10:49:07.187047Z","iopub.status.idle":"2022-12-29T10:49:07.265098Z","shell.execute_reply.started":"2022-12-29T10:49:07.187018Z","shell.execute_reply":"2022-12-29T10:49:07.264407Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_vectors,train_tokenizer = data.word_2_vec(questions)\nlabel_vectors,label_tokenizer = data.word_2_vec(answers)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:07.527541Z","iopub.execute_input":"2022-12-29T10:49:07.528183Z","iopub.status.idle":"2022-12-29T10:49:07.801704Z","shell.execute_reply.started":"2022-12-29T10:49:07.528138Z","shell.execute_reply":"2022-12-29T10:49:07.800755Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"max_length_train = train_vectors.shape[1]\nmax_length_label = label_vectors.shape[1]\nmax_length_label","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:07.905440Z","iopub.execute_input":"2022-12-29T10:49:07.905814Z","iopub.status.idle":"2022-12-29T10:49:07.914309Z","shell.execute_reply.started":"2022-12-29T10:49:07.905783Z","shell.execute_reply":"2022-12-29T10:49:07.913216Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"24"},"metadata":{}}]},{"cell_type":"code","source":"batch_size=64\nbuffer_size = train_vectors.shape[0]\nembedding_dims = 1024\nsteps_per_epoch = buffer_size//batch_size\nunits=1024","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:08.252419Z","iopub.execute_input":"2022-12-29T10:49:08.252774Z","iopub.status.idle":"2022-12-29T10:49:08.257178Z","shell.execute_reply.started":"2022-12-29T10:49:08.252746Z","shell.execute_reply":"2022-12-29T10:49:08.256272Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encoder input shape\nlen(train_tokenizer.word_index)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:09.058157Z","iopub.execute_input":"2022-12-29T10:49:09.059011Z","iopub.status.idle":"2022-12-29T10:49:09.065469Z","shell.execute_reply.started":"2022-12-29T10:49:09.058980Z","shell.execute_reply":"2022-12-29T10:49:09.064531Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"2649"},"metadata":{}}]},{"cell_type":"code","source":"vocab_train = len(train_tokenizer.word_index) +1\nvocab_label = len(label_tokenizer.word_index) +1","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:09.527644Z","iopub.execute_input":"2022-12-29T10:49:09.528582Z","iopub.status.idle":"2022-12-29T10:49:09.532152Z","shell.execute_reply.started":"2022-12-29T10:49:09.528546Z","shell.execute_reply":"2022-12-29T10:49:09.531423Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((train_vectors,label_vectors))\ndataset = dataset.shuffle(buffer_size)\ndataset = dataset.batch(batch_size,drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:09.948286Z","iopub.execute_input":"2022-12-29T10:49:09.948947Z","iopub.status.idle":"2022-12-29T10:49:09.992851Z","shell.execute_reply.started":"2022-12-29T10:49:09.948915Z","shell.execute_reply":"2022-12-29T10:49:09.992121Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_train,embedding_dims,units,batch_size)\ndecoder = Decoder(vocab_label,embedding_dims,units,batch_size)\ntrainer = Train()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:10.387110Z","iopub.execute_input":"2022-12-29T10:49:10.387793Z","iopub.status.idle":"2022-12-29T10:49:10.426293Z","shell.execute_reply.started":"2022-12-29T10:49:10.387756Z","shell.execute_reply":"2022-12-29T10:49:10.425548Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dataset.take(steps_per_epoch)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:10.783984Z","iopub.execute_input":"2022-12-29T10:49:10.784710Z","iopub.status.idle":"2022-12-29T10:49:10.792044Z","shell.execute_reply.started":"2022-12-29T10:49:10.784671Z","shell.execute_reply":"2022-12-29T10:49:10.791270Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<TakeDataset shapes: ((64, 22), (64, 24)), types: (tf.int32, tf.int32)>"},"metadata":{}}]},{"cell_type":"code","source":"EPOCHS = 20\n\nfor epochs in range(1,EPOCHS+1):\n    encoder_hidden = tf.zeros((batch_size,units))\n    total_loss=0\n\nfor i in enumerate(dataset.take(steps_per_epoch)):\n    for (batch_num, (train_data,label_data)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = trainer.Training(train_data,label_data,encoder_hidden,encoder,decoder,batch_size,label_tokenizer)\n        total_loss = total_loss * batch_size\n    \n    print(f\"Epoch:{epochs},Loss:{total_loss/steps_per_epoch}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChatBot:\n    def __init__(self,encoder,decoder,train_tokenizer,label_tokenizer,max_length_train,units):\n        self.train_tokenizer= train_tokenizer\n        self.label_tokenizer = label_tokenizer\n        self.encoder = encoder\n        self.decoder = decoder\n        self.units = units\n        self.data = Data_preprocessing()\n        self.max_len = max_length_train\n        \n    def predict(self,sentence):\n        sentence = self.data.process_sentence(sentence)\n        \n        sentence_mat = []\n        for word in sentence.split(\" \"):\n            try:\n                sentence_mat.append(self.train_tokenizer.word_index[word])\n            except:\n                return \"Couldn't understand the issue\"\n        \n        sentence_mat = pad_sequences([sentence_mat],maxlen=self.maxlen,padding=\"post\")\n        sentence_mat= tf.convert_to_tensor(sentence_mat)\n        \n        enc_hidden = [tf.zeros((1,self.units))]\n        encoder_outputs,thought_vector = self.encoder(sentence_mat,enc_hidden)\n        \n        dec_hidden = thought_vector\n        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']],0)\n        \n        ans = \"\"\n        for i in range(1,self.max_len):\n            pred,dec_hidden,_ = decoder(dec_input,encoder_outputs,dec_hidden)\n            word = self.label_tokenizer.index_word[np.argmax[pred[0]]]\n            \n            ans.append(word)\n            \n            if word == '<end>':\n                return ans\n            \n            dec_input = tf.expand_dims([np.argmax(pred[0])],0)\n        \n        return ans","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:14.122908Z","iopub.status.idle":"2022-12-29T10:49:14.123248Z","shell.execute_reply.started":"2022-12-29T10:49:14.123066Z","shell.execute_reply":"2022-12-29T10:49:14.123083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bot = ChatBot(encoder,decoder,train_tokenizer,label_tokenizer,max_length_train,units)\n\nquestion = ''\nwhile True:\n    question = str(input('You:'))\n    if question == 'quit' or question == 'Quit':\n        break\n    \n    answer = bot.predict(question)\n    print(f\"Bot: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-29T10:49:14.124729Z","iopub.status.idle":"2022-12-29T10:49:14.125027Z","shell.execute_reply.started":"2022-12-29T10:49:14.124877Z","shell.execute_reply":"2022-12-29T10:49:14.124892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}